import os
import gc
import datetime
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from xgboost import XGBRFRegressor
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
import xgboost as xgb
import warnings
warnings.simplefilter('ignore', FutureWarning)
import heapq
import math

train = pd.read_csv("/content/drive/My Drive/train.csv", parse_dates=["first_active_month"])
test = pd.read_csv("/content/drive/My Drive/test.csv", parse_dates=["first_active_month"])

merchants = pd.read_csv("/content/drive/My Drive/merchants.csv")
historical_transactions = pd.read_csv("/content/drive/My Drive/historical_transactions.csv")
new_merchant_transactions = pd.read_csv("/content/drive/My Drive/new_merchant_transactions.csv")

sample_submission = pd.read_csv("/content/drive/My Drive/sample_submission.csv")


def missing_impute(df):
    for i in df.columns:
        if df[i].dtype == "object":
            df[i] = df[i].fillna("other")
        elif (df[i].dtype == "int64" or df[i].dtype == "float64"):
            df[i] = df[i].fillna(df[i].mean())
        else:
            pass
    return df

def datetime_extract(df, dt_col='first_active_month'): 
    df['elapsed_time'] = (datetime.date(2018, 2, 1) - df[dt_col].dt.date).dt.days

    return df


# Do impute missing values for all datasets
for df in [train, test, merchants, historical_transactions, new_merchant_transactions]:
    missing_impute(df)
    

# Do extract datetime values for train and test
train = datetime_extract(train, dt_col='first_active_month')
test = datetime_extract(test, dt_col='first_active_month')

ohe_df_1 = pd.get_dummies(historical_transactions['category_1'], prefix='category_1')
ohe_df_3 = pd.get_dummies(historical_transactions['category_3'], prefix='category_3')
historical_transactions = pd.concat([historical_transactions,ohe_df_1,ohe_df_3], axis=1)
historical_transactions =historical_transactions.drop(['category_1','category_3','merchant_id','authorized_flag','purchase_date'], axis=1)

aggregations = {
    'city_id': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'installments': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'merchant_category_id': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'month_lag': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'category_2': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'state_id': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'subsector_id': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'category_1_N': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'category_1_Y': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'category_3_A': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'category_3_B': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'category_3_C': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'category_3_other': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median'],
    'purchase_amount': ['sum', 'mean', 'std', 'min', 'max', 'size', 'median']   
}
grouped = historical_transactions.groupby('card_id').agg(aggregations)
rename_list = []
keys = aggregations.keys()
for i in keys:
  for x in range(7):
    rename_list.append(i+'_'+aggregations[i][x])

grouped.columns = grouped.columns.droplevel(level=0)
grouped.columns = rename_list
grouped.reset_index(inplace=True)

feature_historical_train = pd.merge(train, grouped, on="card_id", how="left")
feature_historical_test = pd.merge(test, grouped, on="card_id", how="left")
feature_historical_train = feature_historical_train.drop(['first_active_month','feature_1','feature_2','feature_3','elapsed_time'],axis = 1)
feature_historical_test = feature_historical_test.drop(['first_active_month','feature_1','feature_2','feature_3','elapsed_time'],axis = 1)

x = feature_historical_train.iloc[:,2:]
X = feature_historical_test.iloc[:,2:]


y = feature_historical_train.iloc[:,1]
for i in x.columns: 
  x[i] = x[i].fillna(x[i].mean())
for i in X.columns: 
  X[i] = X[i].fillna(X[i].mean())

def pearson(vector1, vector2):
    n = len(vector1)
    #simple sums
    sum1 = sum(float(vector1[i]) for i in range(n))
    sum2 = sum(float(vector2[i]) for i in range(n))
    #sum up the squares
    sum1_pow = sum([pow(v, 2.0) for v in vector1])
    sum2_pow = sum([pow(v, 2.0) for v in vector2])
    #sum up the products
    p_sum = sum([vector1[i]*vector2[i] for i in range(n)])
    #分子num，分母den
    num = p_sum - (sum1*sum2/n)
    den = math.sqrt((sum1_pow-pow(sum1, 2)/n)*(sum2_pow-pow(sum2, 2)/n))
    if den == 0:
        return 0.0
    return num/den
pearson_list = []
for i in x.columns:
  pearson_list.append(pearson(x[i], y))


max_num_index_list = map(pearson_list.index, heapq.nlargest(2, pearson_list))
index_list = list(max_num_index_list)
feature_name = [column for column in x]
con_feature1 = x.iloc[:,index_list[0]]
con_feature2 = x.iloc[:,index_list[1]]
selected_feature_train = pd.concat([con_feature1,con_feature2],axis=1)

con_feature3 = X[feature_name[index_list[0]]]
con_feature4 = X[feature_name[index_list[1]]]
selected_feature_test = pd.concat([con_feature3,con_feature4],axis=1)
train_contact = pd.concat([train,selected_feature_train],axis=1)
test_contact = pd.concat([test,selected_feature_test],axis=1)

train_contact = train_contact.drop(['first_active_month','card_id'],axis = 1)
test_contact = test_contact.drop(['first_active_month','card_id'],axis = 1)

x_train = train_contact.iloc[:,[0,1,2,4,5,6]]
y_train = train_contact.iloc[:,-4]
x_test = test_contact

excluded_features = ['target']
train_features = [c for c in train_contact.columns if c not in excluded_features]

kfolds = KFold(n_splits=5, shuffle=True, random_state=2018)
importances = pd.DataFrame()
oof_preds = np.zeros(x_train.shape[0])
sub_preds = np.zeros(x_test.shape[0])
X = train_contact.copy()
y = X['target']
for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, y)):
  X_train, y_train = X[train_features].iloc[trn_idx], y.iloc[trn_idx]
  X_valid, y_valid = X[train_features].iloc[val_idx], y.iloc[val_idx]

  model = XGBRFRegressor(learning_rate=1, subsample=0.8, colsample_bynode=0.8)
  model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train), (X_valid, y_valid)],
        verbose=True, eval_metric='rmse',
        early_stopping_rounds=100
    )

  

    
  oof_preds[val_idx] = model.predict(X_valid)
  test_preds = model.predict(x_test[train_features])
  sub_preds += test_preds / kfolds.n_splits
  
    
print(np.sqrt(mean_squared_error(y, oof_preds)))



est_scores = []
n_estimators_list = [10,20,50,100,200,300,400]
for l in n_estimators_list:
  kfolds = KFold(n_splits=5, shuffle=True, random_state=2018)
  importances = pd.DataFrame()
  oof_preds = np.zeros(x_train.shape[0])
  sub_preds = np.zeros(x_test.shape[0])
  X = train_contact.copy()
  y = X['target']
  for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, y)):
    X_train, y_train = X[train_features].iloc[trn_idx], y.iloc[trn_idx]
    X_valid, y_valid = X[train_features].iloc[val_idx], y.iloc[val_idx]

    model =XGBRFRegressor(learning_rate=1, subsample=0.8, colsample_bynode=0.8,n_estimators = l)
    model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train), (X_valid, y_valid)],
        verbose=True, eval_metric='rmse',
        early_stopping_rounds=100
      )
    oof_preds[val_idx] = model.predict(X_valid)
    
  est_scores.append(np.sqrt(mean_squared_error(y, oof_preds)))
plt.figure(1)
plt.plot(n_estimators_list, est_scores, 'o-')
plt.ylabel("RMSE")
plt.xlabel("n_estimators")
print("best n_estimators {}".format(n_estimators_list[np.argmin(est_scores)]))
print(min(est_scores))

max_depth_scores = []
max_depth_list = [1,2,3,4,5]
for l in max_depth_list:
  kfolds = KFold(n_splits=5, shuffle=True, random_state=2018)
  importances = pd.DataFrame()
  oof_preds = np.zeros(x_train.shape[0])
  sub_preds = np.zeros(x_test.shape[0])
  X = train_contact.copy()
  y = X['target']
  for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, y)):
    X_train, y_train = X[train_features].iloc[trn_idx], y.iloc[trn_idx]
    X_valid, y_valid = X[train_features].iloc[val_idx], y.iloc[val_idx]

    model =XGBRFRegressor(learning_rate=1, subsample=0.8, colsample_bynode=0.8,n_estimators = n_estimators_list[np.argmin(est_scores)], max_depth = l)
    model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train), (X_valid, y_valid)],
        verbose=True, eval_metric='rmse',
        early_stopping_rounds=100
      )
    oof_preds[val_idx] = model.predict(X_valid)
    
  max_depth_scores.append(np.sqrt(mean_squared_error(y, oof_preds)))
plt.figure(2)
plt.plot(max_depth_list, max_depth_scores, 'o-')
plt.ylabel("RMSE")
plt.xlabel("max_depth")
print("best max_depth {}".format(max_depth_list[np.argmin(max_depth_scores)]))
print(min(max_depth_scores))

lr_scores = []
learning_rates = [0.94,0.96,0.98,1.0,1.02,1.04,1.06]
for l in learning_rates:
  kfolds = KFold(n_splits=5, shuffle=True, random_state=2018)
  importances = pd.DataFrame()
  oof_preds = np.zeros(x_train.shape[0])
  sub_preds = np.zeros(x_test.shape[0])
  X = train_contact.copy()
  y = X['target']
  for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X, y)):
    X_train, y_train = X[train_features].iloc[trn_idx], y.iloc[trn_idx]
    X_valid, y_valid = X[train_features].iloc[val_idx], y.iloc[val_idx]

    model =XGBRFRegressor(learning_rate=l, subsample=0.8, colsample_bynode=0.8,n_estimators = n_estimators_list[np.argmin(est_scores)],max_depth =max_depth_list[np.argmin(max_depth_scores)])
    model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train), (X_valid, y_valid)],
        verbose=True, eval_metric='rmse',
        early_stopping_rounds=100
      )
    oof_preds[val_idx] = model.predict(X_valid)
  lr_scores.append(np.sqrt(mean_squared_error(y, oof_preds)))
plt.figure(3)
plt.plot(learning_rates, lr_scores, 'o-')
plt.ylabel("RMSE")
plt.xlabel("learning_rate")
plt.ylim((3.84, 3.845))
print("best learning_rate {}".format(learning_rates[np.argmin(lr_scores)]))
print(min(lr_scores))
